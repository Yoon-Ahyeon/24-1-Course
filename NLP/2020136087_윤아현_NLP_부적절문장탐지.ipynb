{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqWtPyOjM2tl",
        "outputId": "1e72c98e-a259-482b-9f7b-b39e97b413b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Y0mTQYovyZRF"
      },
      "outputs": [],
      "source": [
        "# !pip install torch # torch\n",
        "# !pip install peft # necessary for finetuning of the large model via LoRA approach\n",
        "# !pip install -i https://pypi.org/simple/ bitsandbytes  # necessary for quantiziation\n",
        "# !pip install evaluate # extension of the transformers library\n",
        "# !pip install datasets # extension of the transformers library\n",
        "# !pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvUSM_rkNYz1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYfwfsqZNQWz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import argparse\n",
        "import torch.nn as nn\n",
        "from tqdm import trange, tqdm\n",
        "# from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import (\n",
        "    AutoModel,\n",
        "    AutoConfig,\n",
        "    XLMRobertaModel,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback,\n",
        "    DataCollatorWithPadding)\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import AdamW\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPXkZwLdNaQH"
      },
      "outputs": [],
      "source": [
        "PADDING_TOKEN = 1\n",
        "S_OPEN_TOKEN = 0\n",
        "S_CLOSE_TOKEN = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8DbYLsXNgXh",
        "outputId": "87706278-b04e-4fd3-f93e-b7221fbe5428"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device:  cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('device: ', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZNbXw91NbSd"
      },
      "outputs": [],
      "source": [
        "labels = ['POSITIVE', 'NEGATIVE']\n",
        "id2label = {idx: label for idx, label in enumerate(labels)}\n",
        "label2id = {label: idx for idx, label in enumerate(labels)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0G3uTjjNdnR"
      },
      "outputs": [],
      "source": [
        "def jsonload(fname, encoding=\"utf-8\"):\n",
        "    with open(fname, encoding=encoding) as f:\n",
        "        j = json.load(f)\n",
        "    return j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ISSq3PuNlJu"
      },
      "outputs": [],
      "source": [
        "# json 개체를 파일이름으로 깔끔하게 저장\n",
        "def jsondump(j, fname):\n",
        "    with open(fname, \"w\", encoding=\"UTF8\") as f:\n",
        "        json.dump(j, f, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hs9As9rlNmQr"
      },
      "outputs": [],
      "source": [
        "# jsonl 파일 읽어서 list에 저장\n",
        "def jsonlload(fname, encoding=\"utf-8\"):\n",
        "    json_list = []\n",
        "    with open(fname, encoding=encoding) as f:\n",
        "        for line in f.readlines():\n",
        "            json_list.append(json.loads(line))\n",
        "    return json_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGbZX9cBNnYV"
      },
      "outputs": [],
      "source": [
        "# jsonlist를 jsonl 형태로 저장\n",
        "def jsonldump(j_list, fname):\n",
        "    f = open(fname, \"w\", encoding='utf-8')\n",
        "    for json_data in j_list:\n",
        "        f.write(json.dumps(json_data, ensure_ascii=False) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zh0RNt65X_yv"
      },
      "outputs": [],
      "source": [
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"unethical expression classifier using pretrained model\")\n",
        "    parser.add_argument(\n",
        "        \"--train_data\", type=str, default=\"/content/drive/MyDrive/NLP/data/nikluge-iau-2023-train.jsonl\",\n",
        "        help=\"train file\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--test_data\", type=str, default=\"/content/drive/MyDrive/NLP/data/nikluge-iau-2023-test.jsonl\",\n",
        "        help=\"test file\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--pred_data\", type=str, default= \"/content/drive/MyDrive/NLP/output/result_kcelec1.jsonl\", # 변경 필요\n",
        "        help=\"pred file\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--dev_data\", type=str, default=\"/content/drive/MyDrive/NLP/data/nikluge-iau-2023-dev.jsonl\",\n",
        "        help=\"dev file\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--batch_size\", type=int, default=8\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--learning_rate\", type=float, default=3e-5\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--eps\", type=float, default=1e-8\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--do_train\", action=\"store_true\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--do_eval\", action=\"store_true\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--do_test\", action=\"store_true\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--num_train_epochs\", type=int, default=8\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--base_model\", type=str, default= \"beomi/KcELECTRA-base\" ## model 변경 필요\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--access_token\", type=str, default=\"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\" # special token\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_path\", type=str, default=\"/content/drive/MyDrive/NLP/save_models/kcelec/saved_modelv2_epoch_8.pt\" # demo 및 test시 변경\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output_dir\", type=str, default=\"/content/drive/MyDrive/NLP/output/\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--do_demo\", action=\"store_true\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max_len\", type=int, default= 256\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--classifier_hidden_size\", type=int, default=768\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--classifier_dropout_prob\", type=int, default=0.1, help=\"dropout in classifier\"\n",
        "    )\n",
        "    args, unknowns = parser.parse_known_args()\n",
        "    return args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mj3LgWX2pOoA"
      },
      "outputs": [],
      "source": [
        "args = parse_args()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9pLEj5G-TvH"
      },
      "source": [
        "### 1번. 데이터 전처리 수행하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlHn5Yzh-TG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9d7d345-9b61-4231-acf7-32dd73ca89b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': 'nikluge-2023-iau-train-000001', 'input': '존나웃기다ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 마술사가 꿈이싣겨죠?', 'output': 'POSITIVE'}, {'id': 'nikluge-2023-iau-train-000002', 'input': '마간호사 존나멋있고 존나웃겨', 'output': 'POSITIVE'}, {'id': 'nikluge-2023-iau-train-000003', 'input': '가던말던니좆대로해~~', 'output': 'NEGATIVE'}, {'id': 'nikluge-2023-iau-train-000004', 'input': '진짜 존나 무기력하다 큰일남', 'output': 'NEGATIVE'}, {'id': 'nikluge-2023-iau-train-000005', 'input': '미친 &name&', 'output': 'NEGATIVE'}, {'id': 'nikluge-2023-iau-train-000006', 'input': 'b조식은 좃같앗는뎅 ㅎㅋㅋㄱㅎㅋ', 'output': 'NEGATIVE'}, {'id': 'nikluge-2023-iau-train-000007', 'input': '개 휘둘린다 ..', 'output': 'NEGATIVE'}, {'id': 'nikluge-2023-iau-train-000008', 'input': '아 시퐈 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ', 'output': 'NEGATIVE'}, {'id': 'nikluge-2023-iau-train-000009', 'input': '#&company& 은 뭐가 그리 무서워서 노조 하나 못 만들게 하는지', 'output': 'NEGATIVE'}, {'id': 'nikluge-2023-iau-train-000010', 'input': '치명적인 뒤태..', 'output': 'POSITIVE'}]\n"
          ]
        }
      ],
      "source": [
        "example = jsonlload(args.train_data)\n",
        "print(example[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcTOZYGoODTY"
      },
      "outputs": [],
      "source": [
        "# data token화를 통해 훈련용 dataset 생성\n",
        "def tokenize_and_align_labels(tokenizer, input_text, label, max_len):\n",
        "    data_dict = {\n",
        "        'input_ids': [],\n",
        "        'attention_mask': [],\n",
        "        'label': [],\n",
        "    }\n",
        "    tokenized_data = tokenizer(input_text, padding='max_length', max_length=max_len, truncation=True)\n",
        "    data_dict['input_ids'].append(tokenized_data['input_ids'])\n",
        "    data_dict['attention_mask'].append(tokenized_data['attention_mask'])\n",
        "    data_dict['label'].append(label)\n",
        "\n",
        "    return data_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qr2-2ogAOasg"
      },
      "outputs": [],
      "source": [
        "def get_dataset(raw_data, tokenizer, max_len):\n",
        "    input_ids_list = []\n",
        "    attention_mask_list = []\n",
        "    token_labels_list = []\n",
        "\n",
        "    for utterance in raw_data:\n",
        "        tokenized_data = tokenize_and_align_labels(tokenizer,\n",
        "                                                   utterance['input'],\n",
        "                                                   label2id[utterance['output']],\n",
        "                                                   max_len)\n",
        "        input_ids_list.extend(tokenized_data['input_ids'])\n",
        "        attention_mask_list.extend(tokenized_data['attention_mask'])\n",
        "        token_labels_list.extend(tokenized_data['label'])\n",
        "\n",
        "    print(input_ids_list[:5])\n",
        "    print(attention_mask_list[:5])\n",
        "    print(token_labels_list[:5])\n",
        "\n",
        "    return TensorDataset(torch.tensor(input_ids_list), torch.tensor(attention_mask_list),\n",
        "                         torch.tensor(token_labels_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2DLOkTj_Pjo"
      },
      "source": [
        "### 2번. Classication Model 생성하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4hgDyDmYC6s"
      },
      "outputs": [],
      "source": [
        "class SimpleClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, args, num_label):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(args.classifier_hidden_size, args.classifier_hidden_size)\n",
        "        self.dropout = nn.Dropout(args.classifier_dropout_prob)\n",
        "        self.output = nn.Linear(args.classifier_hidden_size, num_label)\n",
        "\n",
        "    def forward(self, features):\n",
        "        x = features[:, 0, :]\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZMiCb4K7ZNu"
      },
      "outputs": [],
      "source": [
        "class CustomClassifier(nn.Module):\n",
        "    def __init__(self, args, num_labels, len_tokenizer):\n",
        "        super(CustomClassifier, self).__init__()\n",
        "\n",
        "        # quantization_config = BitsAndBytesConfig(\n",
        "        #     load_in_4bit=True,\n",
        "        # )\n",
        "\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "        config = AutoConfig.from_pretrained(\n",
        "            args.base_model,\n",
        "            num_labels=num_labels)\n",
        "        print(config)\n",
        "\n",
        "        self.pre_trained_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            args.base_model,\n",
        "            token=args.access_token,\n",
        "            config=config,\n",
        "            ignore_mismatched_sizes=True,\n",
        "            # quantization_config=quantization_config  # Uncomment if you want to use quantization\n",
        "        )\n",
        "\n",
        "        self.pre_trained_model.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.simple_classifier = SimpleClassifier(args, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.pre_trained_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels),\n",
        "                            labels.view(-1))\n",
        "\n",
        "        return loss, logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXQEv2VwOlxh"
      },
      "outputs": [],
      "source": [
        "def evaluation(y_true, y_pred):\n",
        "    # y_true = list(map(int, y_true))\n",
        "    # y_pred = list(map(int, y_pred))\n",
        "\n",
        "    print(y_true[:5])\n",
        "    print(y_pred[:5])\n",
        "\n",
        "    print('f1_score: ', f1_score(y_true, y_pred, average=None))\n",
        "    print('f1_score_micro: ', f1_score(y_true, y_pred, average='micro'))\n",
        "    print('f1_score_macro: ', f1_score(y_true, y_pred, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8K7vmcdw74JN"
      },
      "outputs": [],
      "source": [
        "## 분류기 훈련\n",
        "def train(args=None):\n",
        "    if not os.path.exists(args.model_path):\n",
        "        os.makedirs(args.model_path)\n",
        "\n",
        "    print('train')\n",
        "    print('model would be saved at ', args.model_path)\n",
        "\n",
        "    print('loading train data')\n",
        "    ## json으로 data를 load한다.\n",
        "    train_data = jsonlload(args.train_data)\n",
        "    dev_data = jsonlload(args.dev_data)\n",
        "\n",
        "    print('tokenizing train data')\n",
        "    ## base model을 기반으로 토크나이저 초기화\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.base_model)\n",
        "\n",
        "\n",
        "    ## dataloader 설정 - 모델 입력 형식에 맞게 변환\n",
        "    train_dataloader = DataLoader(get_dataset(train_data, tokenizer, args.max_len), shuffle=True,\n",
        "                                  batch_size=args.batch_size)\n",
        "    dev_dataloader = DataLoader(get_dataset(dev_data, tokenizer, args.max_len), shuffle=True,\n",
        "                                batch_size=args.batch_size)\n",
        "\n",
        "    ## model 초기화 및 optimizer 설정\n",
        "    print('loading model')\n",
        "    model = CustomClassifier(args, num_labels=2, len_tokenizer=len(tokenizer))\n",
        "    model.to(device)\n",
        "\n",
        "    # print(model)\n",
        "\n",
        "    FULL_FINETUNING = True\n",
        "    if FULL_FINETUNING:\n",
        "        param_optimizer = list(model.named_parameters())\n",
        "        no_decay = ['bias', 'gamma', 'beta']\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.01},\n",
        "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.0}\n",
        "        ]\n",
        "    else:\n",
        "        param_optimizer = list(model.classifier.named_parameters())\n",
        "        optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        optimizer_grouped_parameters,\n",
        "        lr=args.learning_rate,\n",
        "        eps=args.eps\n",
        "    )\n",
        "    epochs = args.num_train_epochs\n",
        "    max_grad_norm = 1.0\n",
        "    total_steps = epochs * len(train_dataloader)\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    epoch_step = 0\n",
        "\n",
        "    for _ in trange(epochs, desc=\"Epoch\"):\n",
        "        model.train()\n",
        "        epoch_step += 1\n",
        "        total_loss = 0\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            model.zero_grad()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss, _ = model(b_input_ids, b_input_mask, b_labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # print('batch_loss: ', loss.item())\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "        print(\"Epoch: \", epoch_step)\n",
        "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "\n",
        "        if args.do_eval:\n",
        "            model.eval()\n",
        "\n",
        "            pred_list = []\n",
        "            label_list = []\n",
        "\n",
        "            for batch in dev_dataloader:\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    loss, logits = model(b_input_ids, b_input_mask, b_labels)\n",
        "\n",
        "                predictions = torch.argmax(logits, dim=-1)\n",
        "                pred_list.extend(predictions)\n",
        "                label_list.extend(b_labels)\n",
        "\n",
        "            evaluation(label_list, pred_list)\n",
        "\n",
        "        model_saved_path = args.model_path + 'saved_modelv2_epoch_' + str(epoch_step) + '.pt'\n",
        "        torch.save(model.state_dict(), model_saved_path)\n",
        "\n",
        "    print(\"training is done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWrhxNJsOvni"
      },
      "outputs": [],
      "source": [
        "def test(args):\n",
        "    test_data = jsonlload(args.test_data)\n",
        "    pred_data = jsonlload(args.pred_data)\n",
        "\n",
        "    classes = []\n",
        "\n",
        "    temp_ground_truth_dict = {}\n",
        "\n",
        "    true_list = []\n",
        "    pred_list = []\n",
        "\n",
        "    # 데이터 list로 변경\n",
        "    for data in test_data:\n",
        "        if data['id'] in temp_ground_truth_dict:\n",
        "            return {\n",
        "                \"error\": \"정답 데이터에 중복된 id를 가지는 경우 존재\"\n",
        "            }\n",
        "        temp_ground_truth_dict[data['id']] = data['output']\n",
        "\n",
        "    for data in pred_data:\n",
        "        if data['id'] not in temp_ground_truth_dict:\n",
        "            return {\n",
        "                \"error\": \"제출 파일과 정답 파일의 id가 일치하지 않음\"\n",
        "            }\n",
        "        true_list.append(temp_ground_truth_dict[data['id']])\n",
        "        pred_list.append(data['output'])\n",
        "\n",
        "    evaluation(true_list, pred_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vf0hF-1wYblG"
      },
      "outputs": [],
      "source": [
        "def demo(args):\n",
        "    if not os.path.exists(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.base_model)\n",
        "\n",
        "    test_data = jsonlload(args.test_data)\n",
        "\n",
        "    model = CustomClassifier(args, len(labels), len(tokenizer))\n",
        "    model.load_state_dict(torch.load(args.model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    for data in tqdm(test_data):\n",
        "        input_text = data['input']\n",
        "\n",
        "        tokenized_data = tokenizer(input_text, padding='max_length', max_length=args.max_len, truncation=True)\n",
        "\n",
        "        input_ids = torch.tensor([tokenized_data['input_ids']]).to(device)\n",
        "        attention_mask = torch.tensor([tokenized_data['attention_mask']]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, logits = model(input_ids, attention_mask)\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        data['output'] = id2label[int(predictions[0])]\n",
        "\n",
        "    jsonldump(test_data, args.output_dir + 'result_kcelec1.jsonl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAF8oiH0r1LO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fda4f2f4-235e-41f7-ee32-b682a060319f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(train_data='/content/drive/MyDrive/NLP/data/nikluge-iau-2023-train.jsonl', test_data='/content/drive/MyDrive/NLP/data/nikluge-iau-2023-test.jsonl', pred_data='/content/drive/MyDrive/NLP/output/result_kcelec2.jsonl', dev_data='/content/drive/MyDrive/NLP/data/nikluge-iau-2023-dev.jsonl', batch_size=8, learning_rate=3e-05, eps=1e-08, do_train=False, do_eval=False, do_test=False, num_train_epochs=8, base_model='beomi/KcELECTRA-base', access_token='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', model_path='/content/drive/MyDrive/NLP/save_models/kcelec/saved_modelv2_epoch_6.pt', output_dir='/content/drive/MyDrive/NLP/output/', do_demo=False, max_len=256, classifier_hidden_size=768, classifier_dropout_prob=0.1)\n"
          ]
        }
      ],
      "source": [
        "print(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnO7Qsf1pYY7"
      },
      "outputs": [],
      "source": [
        "train(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcByrbU3q2mO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9818cdb-ccb2-4a1c-af46-23aa5a456eac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElectraConfig {\n",
            "  \"_name_or_path\": \"beomi/KcELECTRA-base\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|██████████| 1624/1624 [00:27<00:00, 59.95it/s]\n"
          ]
        }
      ],
      "source": [
        "demo(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKPssoBsq8_Z"
      },
      "outputs": [],
      "source": [
        "test(args)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uGKbs5-3x1EB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}