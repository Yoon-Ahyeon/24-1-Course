{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 크기를 줄여서 4bit로 사용한다.\n",
    "#hugging-face를 사용하여 작은 chatgpt를 사용할 수 있다. \n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCasualLM\n",
    "\n",
    "MODEL = \"beomi/KoAlpaca-llama-1-7b\"\n",
    "\n",
    "#사전 학습 모델 로드\n",
    "model = AutoModelForCasualLM.from_pretrained(\n",
    "    MODEL,\n",
    "    load_in_4bit = True,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "model.eval() #모델을 평가 모드로 설정 (학습 X, 사용만)\n",
    "\n",
    "pipe = pipeline(\n",
    "    'text-generation', #텍스트 생성 task 지정\n",
    "    model = model, #사전 학습 모델 사용\n",
    "    tokenizer = MODEL #토크나이저도 동일하게 사용\n",
    ")\n",
    "\n",
    "def ask(question, context = '', is_input_full = False):\n",
    "    ans = pipe(\n",
    "        f\"### 질문 : {question}\\n\\n### 맥락 : {context}\\n\\n### 답변 : \" if context else f\"### 질문 : {question}\\n\\n### 답변 :\",\n",
    "        do_sample = True,\n",
    "        max_new_tokens = 512,\n",
    "        temperature = 0.7,\n",
    "        top_p = 0.9,\n",
    "        return_full_text = False,\n",
    "        pad_token_id = 32000,\n",
    "        eos_toke_id = 2,\n",
    "    )\n",
    "    print(ans[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(nlp) C:\\Users\\ahyeo>conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia ->이거 설치 안됨 !! 다시 conda cmd에서 수행 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(\"딥러닝이 뭐야?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
