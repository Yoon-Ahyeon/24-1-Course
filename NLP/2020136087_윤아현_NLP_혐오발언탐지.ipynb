{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCeXXuhU_fFl",
        "outputId": "7b29d838-7f72-41d6-f36c-a6ea12addfd0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import re"
      ],
      "metadata": {
        "id": "sNRtOexJCdac"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import argparse\n",
        "import torch.nn as nn\n",
        "from tqdm import trange, tqdm\n",
        "from transformers import (\n",
        "    AutoModel,\n",
        "    AutoConfig,\n",
        "    BertConfig,\n",
        "    BertModel,\n",
        "    XLMRobertaModel,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback,\n",
        "    DataCollatorWithPadding)\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import AdamW\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "qZCt144aCfUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PADDING_TOKEN = 1\n",
        "S_OPEN_TOKEN = 0\n",
        "S_CLOSE_TOKEN = 2"
      ],
      "metadata": {
        "id": "54723J9SFdNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('device: ', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBUSre-wCj85",
        "outputId": "2b40cfda-857d-40db-a2a8-30519935da74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "special_tokens_dict = {\n",
        "    'additional_special_tokens': ['&name&', '&affiliation&', '&social-security-num&', '&tel-num&', '&card-num&', '&bank-account&', '&num&', '&online-account&']\n",
        "}"
      ],
      "metadata": {
        "id": "B-tph6UGFgAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### json 불러오기"
      ],
      "metadata": {
        "id": "QvQp7Go6FhQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jsonload(fname, encoding=\"utf-8\"):\n",
        "    with open(fname, encoding=encoding) as f:\n",
        "        j = json.load(f)\n",
        "\n",
        "    return j"
      ],
      "metadata": {
        "id": "Dbm3Yh-e-3HJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# json 개체를 파일이름으로 깔끔하게 저장\n",
        "def jsondump(j, fname):\n",
        "    with open(fname, \"w\", encoding=\"UTF8\") as f:\n",
        "        json.dump(j, f, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "eSe-4wum-3Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# jsonl 파일 읽어서 list에 저장\n",
        "def jsonlload(fname, encoding=\"utf-8\"):\n",
        "    json_list = []\n",
        "    with open(fname, encoding=encoding) as f:\n",
        "        for line in f.readlines():\n",
        "            json_list.append(json.loads(line))\n",
        "    return json_list"
      ],
      "metadata": {
        "id": "98kY1pnJCnML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# jsonlist를 jsonl 형태로 저장\n",
        "def jsonldump(j_list, fname):\n",
        "    f = open(fname, \"w\", encoding='utf-8')\n",
        "    for json_data in j_list:\n",
        "        f.write(json.dumps(json_data, ensure_ascii=False)+'\\n')"
      ],
      "metadata": {
        "id": "OByOwLAI-3OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 파싱하기"
      ],
      "metadata": {
        "id": "qR7uND7AFqc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"unethical expression classifier using pretrained model\")\n",
        "    parser.add_argument(\n",
        "        \"--train_data\", type=str, default=\"/content/drive/MyDrive/NLP/data/nikluge-au-2022-train.jsonl\",\n",
        "        help=\"train file\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--test_data\", type=str, default=\"/content/drive/MyDrive/NLP/data/nikluge-au-2022-test.jsonl\",\n",
        "        help=\"test file\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--pred_data\", type=str, default=\"/content/drive/MyDrive/NLP/output/hate_you_6.jsonl\",\n",
        "        help=\"pred file\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--dev_data\", type=str, default=\"/content/drive/MyDrive/NLP/data/nikluge-au-2022-dev.jsonl\",\n",
        "        help=\"dev file\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--batch_size\", type=int, default=8\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--learning_rate\", type=float, default=3e-6\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--eps\", type=float, default=1e-8\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--do_train\", action=\"store_true\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--do_eval\", action=\"store_true\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--do_test\", action=\"store_true\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--num_train_epochs\", type=int, default=6\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--base_model\", type=str, default=\"beomi/korean-hatespeech-classifier\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_path\", type=str, default=\"/content/drive/MyDrive/NLP/save_models/korean-hatespeech-classifier/saved_model_epoch_6.pt\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--access_token\", type=str, default=\"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\" # special token\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output_dir\", type=str, default=\"/content/drive/MyDrive/NLP/output/\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--do_demo\", action=\"store_true\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max_len\", type=int, default=256\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--classifier_hidden_size\", type=int, default=768\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--classifier_dropout_prob\", type=int, default=0.1, help=\"dropout in classifier\"\n",
        "    )\n",
        "    args, unknowns = parser.parse_known_args()\n",
        "    return args"
      ],
      "metadata": {
        "id": "yDUSphP3EE3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = parse_args()"
      ],
      "metadata": {
        "id": "LUEkK66MEGFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 확인하기"
      ],
      "metadata": {
        "id": "dk2ZoMemhG3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = jsonlload(args.train_data)\n",
        "print(example[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VH_sCu0hIaP",
        "outputId": "7f970317-ae46-466c-adcd-db3845f6355c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': 'nikluge-au-2022-train-000001', 'input': '보여주면서 왜 엿보냐고 비난 하는것도 웃기지만. 훔쳐 보면서 왜 보여주냐고 하는 사람 역시 우습다..', 'output': 1}, {'id': 'nikluge-au-2022-train-000002', 'input': '왜 개인 사생활을 방송으로 보여주고 싶은지 이해도 안가지만 &location&식 프로포즈란 무슨 자로 잰 든 무릎 꿇고 반지 내밀고 나랑 결혼해줄래? 가 전부이다.', 'output': 1}, {'id': 'nikluge-au-2022-train-000003', 'input': '이런 쓰레기같은 새끼가 아무렇지 않게 멀쩡히 돌아다닐 생각을 하니까 진짜 너무 소름돋는다.', 'output': 1}, {'id': 'nikluge-au-2022-train-000004', 'input': '인간의 탈을 쓰고...', 'output': 1}, {'id': 'nikluge-au-2022-train-000005', 'input': '인기글에 짱깨뭐라하니까 댓글로 ㅂㄷㅂㄷ하네요...', 'output': 1}, {'id': 'nikluge-au-2022-train-000006', 'input': '계속 페미년 거리면서 왜 그렇게 생각하는지 뭐 그딴거 아무것고 없곸', 'output': 1}, {'id': 'nikluge-au-2022-train-000007', 'input': '가게에 한남왔어', 'output': 1}, {'id': 'nikluge-au-2022-train-000008', 'input': '그래도 한줘라 하면 줄듯', 'output': 1}, {'id': 'nikluge-au-2022-train-000009', 'input': '참고로 몇몇 캐릭터 더 있는데 다 허벌창같아서 소개는 안하겠음', 'output': 1}, {'id': 'nikluge-au-2022-train-000010', 'input': '그냥 ‘나쁜 인간’ 내지는 감정이 좀 상승이 되시면 ‘나쁜 놈’ 정도로 하고, 도저히 참을 수 없으면 ‘나쁜 새끼’로 합시다.', 'output': 1}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataLoader"
      ],
      "metadata": {
        "id": "H_BRNbiHCzlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(tokenizer, form, label, max_len):\n",
        "    data_dict = {\n",
        "          'input_ids': [],\n",
        "          'attention_mask': [],\n",
        "          'label': [],\n",
        "      }\n",
        "\n",
        "    tokenized_data = tokenizer(form,\n",
        "                               padding='max_length',\n",
        "                               max_length=max_len,\n",
        "                               truncation=True,\n",
        "                               add_special_tokens=True)\n",
        "\n",
        "    data_dict['input_ids'].append(tokenized_data['input_ids'])\n",
        "    data_dict['attention_mask'].append(tokenized_data['attention_mask'])\n",
        "    data_dict['label'].append(label)\n",
        "\n",
        "    return data_dict"
      ],
      "metadata": {
        "id": "3n7rvsEAC2k7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(raw_data, tokenizer, max_len):\n",
        "    input_ids_list = []\n",
        "    attention_mask_list = []\n",
        "    token_labels_list = []\n",
        "\n",
        "    for utterance in raw_data:\n",
        "        tokenized_data = tokenize_and_align_labels(tokenizer, utterance['input'], utterance['output'] , max_len)\n",
        "\n",
        "        # if tokenized_data is not None:\n",
        "        input_ids_list.extend(tokenized_data['input_ids'])\n",
        "        attention_mask_list.extend(tokenized_data['attention_mask'])\n",
        "        token_labels_list.extend(tokenized_data['label'])\n",
        "\n",
        "    print(input_ids_list[:5])\n",
        "    print(attention_mask_list[:5])\n",
        "    print(token_labels_list[:5])\n",
        "\n",
        "    return TensorDataset(torch.tensor(input_ids_list), torch.tensor(attention_mask_list),\n",
        "                         torch.tensor(token_labels_list))"
      ],
      "metadata": {
        "id": "OjDvHw0PD7Ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "qp-9akrTAYm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, args, num_label):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(args.classifier_hidden_size, args.classifier_hidden_size)\n",
        "        self.dropout = nn.Dropout(args.classifier_dropout_prob)\n",
        "        self.output = nn.Linear(args.classifier_hidden_size, num_label)\n",
        "\n",
        "    def forward(self, features):\n",
        "        x = features[:, 0, :]\n",
        "        # x = features\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = torch.tanh(x)\n",
        "        # x = torch.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "9LmYK2NK-3VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnethicalExpressionClassifier(nn.Module):\n",
        "    def __init__(self, args, num_label, len_tokenizer):\n",
        "        super(UnethicalExpressionClassifier, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        print(self.num_label)\n",
        "\n",
        "        # config = BertConfig.from_pretrained(\n",
        "        #     args.base_model,\n",
        "        #     num_labels=num_label)\n",
        "        # print(config)\n",
        "\n",
        "        # AutoModel로도 돌려보기..\n",
        "        self.pre_trained_model = AutoModel.from_pretrained(\n",
        "            args.base_model,\n",
        "            token=args.access_token,\n",
        "            # config=config,\n",
        "            ignore_mismatched_sizes=True,\n",
        "        )\n",
        "        self.pre_trained_model.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier(args, self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.pre_trained_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        # print(\"\\nSeqence Output: \", sequence_output.shape)\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label), labels.view(-1))\n",
        "\n",
        "        return loss, logits"
      ],
      "metadata": {
        "id": "u4eEAScYDURM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(y_true, y_pred):\n",
        "\n",
        "    y_true = list(map(int, y_true))\n",
        "    y_pred = list(map(int, y_pred))\n",
        "\n",
        "    print('f1_score: ', f1_score(y_true, y_pred, average=None))\n",
        "    print('f1_score_micro: ', f1_score(y_true, y_pred, average='micro'))"
      ],
      "metadata": {
        "id": "Jc6faqt4D2Kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 훈련하기"
      ],
      "metadata": {
        "id": "bA7JiHBUDufu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_unethical_expression_classifier(args=None):\n",
        "    if not os.path.exists(args.model_path):\n",
        "        os.makedirs(args.model_path)\n",
        "\n",
        "    print('train_unethical_expression_classifier')\n",
        "    print('model would be saved at ', args.model_path)\n",
        "\n",
        "    print('loading train data')\n",
        "    train_data = jsonlload(args.train_data)\n",
        "    dev_data = jsonlload(args.dev_data)\n",
        "\n",
        "    print('tokenizing train data')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.base_model)\n",
        "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    print('We have added', num_added_toks, 'tokens')\n",
        "    train_dataloader = DataLoader(get_dataset(train_data, tokenizer, args.max_len), shuffle=True,\n",
        "                                  batch_size=args.batch_size)\n",
        "    dev_dataloader = DataLoader(get_dataset(dev_data, tokenizer, args.max_len), shuffle=True,\n",
        "                                batch_size=args.batch_size)\n",
        "\n",
        "    print('loading model')\n",
        "    model = UnethicalExpressionClassifier(args, 2, len(tokenizer))\n",
        "    model.to(device)\n",
        "\n",
        "    # print(model)\n",
        "\n",
        "    FULL_FINETUNING = True\n",
        "    if FULL_FINETUNING:\n",
        "        param_optimizer = list(model.named_parameters())\n",
        "        no_decay = ['bias', 'gamma', 'beta']\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.01},\n",
        "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.0}\n",
        "        ]\n",
        "    else:\n",
        "        param_optimizer = list(model.classifier.named_parameters())\n",
        "        optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        optimizer_grouped_parameters,\n",
        "        lr=args.learning_rate,\n",
        "        eps=args.eps\n",
        "    )\n",
        "    epochs = args.num_train_epochs\n",
        "    max_grad_norm = 1.0\n",
        "    total_steps = epochs * len(train_dataloader)\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    epoch_step = 0\n",
        "\n",
        "    for _ in trange(epochs, desc=\"Epoch\"):\n",
        "        model.train()\n",
        "        epoch_step += 1\n",
        "        total_loss = 0\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            loss, _ = model(b_input_ids, b_input_mask, b_labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # print('batch_loss: ', loss.item())\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "        print(\"Epoch: \", epoch_step)\n",
        "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "\n",
        "        if args.do_eval:\n",
        "            model.eval()\n",
        "\n",
        "            pred_list = []\n",
        "            label_list = []\n",
        "\n",
        "            for batch in dev_dataloader:\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    loss, logits = model(b_input_ids, b_input_mask, b_labels)\n",
        "\n",
        "                predictions = torch.argmax(logits, dim=-1)\n",
        "                pred_list.extend(predictions)\n",
        "                label_list.extend(b_labels)\n",
        "\n",
        "            evaluation(label_list, pred_list)\n",
        "\n",
        "        if not os.path.exists(args.model_path):\n",
        "            os.makedirs(args.model_path)\n",
        "\n",
        "        model_saved_path = args.model_path + 'saved_model_epoch_' + str(epoch_step) + '.pt'\n",
        "        torch.save(model.state_dict(), model_saved_path)\n",
        "\n",
        "    print(\"training is done\")"
      ],
      "metadata": {
        "id": "BKtSQykVDtk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_unethical_expression_classifier(args):\n",
        "\n",
        "    test_data = jsonlload(args.test_data)\n",
        "    pred_data = jsonlload(args.pred_data)\n",
        "\n",
        "    temp_ground_truth_dict = {}\n",
        "\n",
        "    true_list = []\n",
        "    pred_list = []\n",
        "\n",
        "    # 데이터 list로 변경\n",
        "    for data in test_data:\n",
        "        if data['id'] in temp_ground_truth_dict:\n",
        "            return {\n",
        "                \"error\": \"정답 데이터에 중복된 id를 가지는 경우 존재\"\n",
        "            }\n",
        "        temp_ground_truth_dict[data['id']] = data['output']\n",
        "\n",
        "    for data in pred_data:\n",
        "        if data['id'] not in temp_ground_truth_dict:\n",
        "            return {\n",
        "                \"error\": \"제출 파일과 정답 파일의 id가 일치하지 않음\"\n",
        "            }\n",
        "        true_list.append(temp_ground_truth_dict[data['id']])\n",
        "        pred_list.append(data['output'])\n",
        "\n",
        "    evaluation(true_list, pred_list)"
      ],
      "metadata": {
        "id": "URV27khHDyYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def separate_by_s_token(form):\n",
        "    splited_temp_form = form.split('</s></s>')\n",
        "    splited_temp_form[0] = splited_temp_form[0].split('<s>')[-1]\n",
        "    splited_temp_form[-1] = splited_temp_form[-1].split('</s>')[0]\n",
        "\n",
        "    for i in range(len(splited_temp_form)):\n",
        "        splited_temp_form[i] = splited_temp_form[i].strip()\n",
        "\n",
        "    return splited_temp_form"
      ],
      "metadata": {
        "id": "8lTt62veGA8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def demo_unethical_expression_classifier(args):\n",
        "    if not os.path.exists(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.base_model)\n",
        "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "    test_data = jsonlload(args.test_data)\n",
        "\n",
        "    model = UnethicalExpressionClassifier(args, 2, len(tokenizer))\n",
        "    model.load_state_dict(torch.load(args.model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    for data in tqdm(test_data):\n",
        "        tokenized_data = tokenizer(data['input'], padding='max_length', max_length=args.max_len, truncation=True)\n",
        "\n",
        "        input_ids = torch.tensor([tokenized_data['input_ids']]).to(device)\n",
        "        attention_mask = torch.tensor([tokenized_data['attention_mask']]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, logits = model(input_ids, attention_mask)\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        data['output'] = int(predictions[0])\n",
        "\n",
        "    jsonldump(test_data, args.output_dir + 'hate_you_6.jsonl')"
      ],
      "metadata": {
        "id": "ADXQ9IOoGCT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_unethical_expression_classifier(args)"
      ],
      "metadata": {
        "id": "fE9ILMEPGECt",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_unethical_expression_classifier(args)"
      ],
      "metadata": {
        "id": "eeRtZfHeGFD_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0c659d2-8340-426e-92f3-87329c6acaad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2072/2072 [00:21<00:00, 97.41it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_unethical_expression_classifier(args)"
      ],
      "metadata": {
        "id": "RG_tvkaRGFG-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}